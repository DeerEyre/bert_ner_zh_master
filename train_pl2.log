/etc/host.conf: line 4: bad command `151.101.76.133 raw.githubusercontent.com '
Some weights of the model checkpoint at hfl/chinese-bert-wwm-ext were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
数据大小： (147, 10)
GPU available: True (cuda), used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1766: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=8)`.
  rank_zero_warn(
/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:115: UserWarning: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.
  rank_zero_warn("You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.")

  | Name       | Type      | Params
-----------------------------------------
0 | pretrained | BertModel | 102 M 
1 | NER        | NER       | 105 M 
-----------------------------------------
105 M     Trainable params
0         Non-trainable params
105 M     Total params
423.266   Total estimated model params size (MB)
/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:219: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 40 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1894: PossibleUserWarning: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
  rank_zero_warn(
Training: 0it [00:00, ?it/s]Training:   0%|          | 0/7 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/7 [00:00<?, ?it/s] train_loss tensor(1.9408, grad_fn=<NllLossBackward0>)
Epoch 0:  14%|█▍        | 1/7 [00:07<00:46,  7.81s/it]Epoch 0:  14%|█▍        | 1/7 [00:07<00:46,  7.81s/it, loss=1.94, v_num=4]train_loss tensor(1.7942, grad_fn=<NllLossBackward0>)
Epoch 0:  29%|██▊       | 2/7 [00:15<00:38,  7.69s/it, loss=1.94, v_num=4]Epoch 0:  29%|██▊       | 2/7 [00:15<00:38,  7.70s/it, loss=1.87, v_num=4]train_loss tensor(1.7170, grad_fn=<NllLossBackward0>)
Epoch 0:  43%|████▎     | 3/7 [00:22<00:30,  7.63s/it, loss=1.87, v_num=4]Epoch 0:  43%|████▎     | 3/7 [00:22<00:30,  7.63s/it, loss=1.82, v_num=4]train_loss tensor(1.7737, grad_fn=<NllLossBackward0>)
Epoch 0:  57%|█████▋    | 4/7 [00:30<00:22,  7.57s/it, loss=1.82, v_num=4]Epoch 0:  57%|█████▋    | 4/7 [00:30<00:22,  7.57s/it, loss=1.81, v_num=4]train_loss tensor(1.6137, grad_fn=<NllLossBackward0>)
Epoch 0:  71%|███████▏  | 5/7 [00:37<00:15,  7.54s/it, loss=1.81, v_num=4]Epoch 0:  71%|███████▏  | 5/7 [00:37<00:15,  7.54s/it, loss=1.77, v_num=4]train_loss tensor(1.5620, grad_fn=<NllLossBackward0>)
Epoch 0:  86%|████████▌ | 6/7 [00:45<00:07,  7.53s/it, loss=1.77, v_num=4]Epoch 0:  86%|████████▌ | 6/7 [00:45<00:07,  7.53s/it, loss=1.73, v_num=4]train_loss tensor(1.6510, grad_fn=<NllLossBackward0>)
Epoch 0: 100%|██████████| 7/7 [00:52<00:00,  7.48s/it, loss=1.73, v_num=4]Epoch 0: 100%|██████████| 7/7 [00:52<00:00,  7.48s/it, loss=1.72, v_num=4]Epoch 0: 100%|██████████| 7/7 [00:52<00:00,  7.48s/it, loss=1.72, v_num=4]Epoch 0:   0%|          | 0/7 [00:00<?, ?it/s, loss=1.72, v_num=4]        Epoch 1:   0%|          | 0/7 [00:00<?, ?it/s, loss=1.72, v_num=4]train_loss tensor(1.6220, grad_fn=<NllLossBackward0>)
Epoch 1:  14%|█▍        | 1/7 [00:07<00:43,  7.28s/it, loss=1.72, v_num=4]Epoch 1:  14%|█▍        | 1/7 [00:07<00:43,  7.28s/it, loss=1.71, v_num=4]train_loss tensor(1.7014, grad_fn=<NllLossBackward0>)
Epoch 1:  29%|██▊       | 2/7 [00:14<00:36,  7.30s/it, loss=1.71, v_num=4]Epoch 1:  29%|██▊       | 2/7 [00:14<00:36,  7.30s/it, loss=1.71, v_num=4]train_loss tensor(1.6887, grad_fn=<NllLossBackward0>)
Epoch 1:  43%|████▎     | 3/7 [00:21<00:29,  7.33s/it, loss=1.71, v_num=4]Epoch 1:  43%|████▎     | 3/7 [00:21<00:29,  7.33s/it, loss=1.71, v_num=4]train_loss tensor(1.7819, grad_fn=<NllLossBackward0>)
Epoch 1:  57%|█████▋    | 4/7 [00:29<00:21,  7.28s/it, loss=1.71, v_num=4]Epoch 1:  57%|█████▋    | 4/7 [00:29<00:21,  7.28s/it, loss=1.71, v_num=4]train_loss tensor(1.6486, grad_fn=<NllLossBackward0>)
Epoch 1:  71%|███████▏  | 5/7 [00:36<00:14,  7.29s/it, loss=1.71, v_num=4]Epoch 1:  71%|███████▏  | 5/7 [00:36<00:14,  7.29s/it, loss=1.71, v_num=4]train_loss tensor(1.6037, grad_fn=<NllLossBackward0>)
Epoch 1:  86%|████████▌ | 6/7 [00:43<00:07,  7.33s/it, loss=1.71, v_num=4]Epoch 1:  86%|████████▌ | 6/7 [00:43<00:07,  7.33s/it, loss=1.7, v_num=4] train_loss tensor(1.6292, grad_fn=<NllLossBackward0>)
Epoch 1: 100%|██████████| 7/7 [00:51<00:00,  7.34s/it, loss=1.7, v_num=4]Epoch 1: 100%|██████████| 7/7 [00:51<00:00,  7.34s/it, loss=1.69, v_num=4]Epoch 1: 100%|██████████| 7/7 [00:51<00:00,  7.34s/it, loss=1.69, v_num=4]Epoch 1:   0%|          | 0/7 [00:00<?, ?it/s, loss=1.69, v_num=4]        Epoch 2:   0%|          | 0/7 [00:00<?, ?it/s, loss=1.69, v_num=4]train_loss tensor(1.6935, grad_fn=<NllLossBackward0>)
Epoch 2:  14%|█▍        | 1/7 [00:07<00:45,  7.51s/it, loss=1.69, v_num=4]Epoch 2:  14%|█▍        | 1/7 [00:07<00:45,  7.51s/it, loss=1.69, v_num=4]train_loss tensor(1.6433, grad_fn=<NllLossBackward0>)
Epoch 2:  29%|██▊       | 2/7 [00:14<00:36,  7.39s/it, loss=1.69, v_num=4]Epoch 2:  29%|██▊       | 2/7 [00:14<00:36,  7.39s/it, loss=1.69, v_num=4]train_loss tensor(1.7140, grad_fn=<NllLossBackward0>)
Epoch 2:  43%|████▎     | 3/7 [00:22<00:29,  7.44s/it, loss=1.69, v_num=4]Epoch 2:  43%|████▎     | 3/7 [00:22<00:29,  7.44s/it, loss=1.69, v_num=4]train_loss tensor(1.8307, grad_fn=<NllLossBackward0>)
Epoch 2:  57%|█████▋    | 4/7 [00:30<00:22,  7.55s/it, loss=1.69, v_num=4]Epoch 2:  57%|█████▋    | 4/7 [00:30<00:22,  7.55s/it, loss=1.7, v_num=4] train_loss tensor(1.6596, grad_fn=<NllLossBackward0>)
Epoch 2:  71%|███████▏  | 5/7 [00:38<00:15,  7.74s/it, loss=1.7, v_num=4]Epoch 2:  71%|███████▏  | 5/7 [00:38<00:15,  7.74s/it, loss=1.7, v_num=4]train_loss tensor(1.6457, grad_fn=<NllLossBackward0>)
Epoch 2:  86%|████████▌ | 6/7 [00:46<00:07,  7.81s/it, loss=1.7, v_num=4]Epoch 2:  86%|████████▌ | 6/7 [00:46<00:07,  7.81s/it, loss=1.7, v_num=4]train_loss tensor(1.5756, grad_fn=<NllLossBackward0>)
Epoch 2: 100%|██████████| 7/7 [00:54<00:00,  7.84s/it, loss=1.7, v_num=4]Epoch 2: 100%|██████████| 7/7 [00:54<00:00,  7.84s/it, loss=1.68, v_num=4]Epoch 2: 100%|██████████| 7/7 [00:54<00:00,  7.84s/it, loss=1.68, v_num=4]`Trainer.fit` stopped: `max_epochs=3` reached.
Epoch 2: 100%|██████████| 7/7 [00:55<00:00,  7.97s/it, loss=1.68, v_num=4]2022/08/27 10:31:48 WARNING mlflow.utils.requirements_utils: Found torch version (1.12.0+cu113) contains a local version label (+cu113). MLflow logged a pip requirement for this package as 'torch==1.12.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

training has been ended
